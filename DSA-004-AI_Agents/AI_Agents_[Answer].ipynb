{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnsa58no87Yj"
      },
      "source": [
        "### Step 1: Import Packages\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "-   Installs all Python libraries required to build an AI agent with LangChain\n",
        "-   Imports the core modules for agents, tools, LLMs, and the Gradio interface\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **LangChain**: Framework for building applications powered by language models; provides abstractions for agents, tools, and chains\n",
        "-   **langchain-openai**: LangChain's integration with OpenAI's GPT models\n",
        "-   **Gradio**: Library for quickly creating web UIs to demo ML models\n",
        "-   **ChromaDB**: Vector database for semantic search (used in some RAG patterns)\n",
        "-   **tiktoken**: OpenAI's tokenizer library for counting tokens\n",
        "-   **pypdf / pdf2image / Pillow**: Libraries for parsing and rendering PDF documents\n",
        "\n",
        "**Guardrails & best practices:**\n",
        "\n",
        "-   Pin library versions to avoid breaking changes mid-course\n",
        "-   Import only what you need to keep namespaces clean\n",
        "-   Use `google.colab.userdata` (Colab Secrets) to store API keys---never hardcode them\n",
        "\n",
        "**Why these imports matter:**\n",
        "\n",
        "-   `ChatOpenAI` → the LLM that powers your agent\n",
        "-   `tool` decorator → converts Python functions into agent-callable tools\n",
        "-   `create_openai_tools_agent` → factory function that wires tools + LLM + prompt\n",
        "-   `AgentExecutor` → runtime loop that invokes the agent and executes tools\n",
        "-   `SystemMessage`, `ChatPromptTemplate` → structures for agent instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eW8ouU25cycv",
        "outputId": "8e319f43-05f8-4592-800a-7138c51308b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain==0.1.16 in /usr/local/lib/python3.12/dist-packages (0.1.16)\n",
            "Requirement already satisfied: langchain-core==0.1.45 in /usr/local/lib/python3.12/dist-packages (0.1.45)\n",
            "Requirement already satisfied: langchain-openai==0.1.3 in /usr/local/lib/python3.12/dist-packages (0.1.3)\n",
            "Requirement already satisfied: openai>=1.0.0 in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting openai>=1.0.0\n",
            "  Using cached openai-2.7.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: gradio>=4.0.0 in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: chromadb<0.5.0 in /usr/local/lib/python3.12/dist-packages (0.4.24)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0 in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: pypdf<4.0.0 in /usr/local/lib/python3.12/dist-packages (3.17.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: transformers<5.0.0 in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: python-dotenv<1.1.0 in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pdf2image<1.17.0 in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: pillow<11.0.0 in /usr/local/lib/python3.12/dist-packages (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (3.13.1)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (0.0.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (0.0.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (2.11.10)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.1.16) (8.5.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core==0.1.45) (23.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.0.0) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.120.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.6.4)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (3.11.4)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (2.2.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.14.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.49.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=4.0.0) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio>=4.0.0) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio>=4.0.0) (15.0.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.3.0)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (0.7.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (6.8.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (3.8.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (0.59b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.38.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (5.0.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb<0.5.0) (5.2.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers<3.0.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers<3.0.0) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers<3.0.0) (1.16.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0) (3.20.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0) (0.6.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (3.11)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb<0.5.0) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (0.9.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio>=4.0.0) (0.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio>=4.0.0) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (3.0.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb<0.5.0) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb<0.5.0) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb<0.5.0) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0) (1.38.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (0.59b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (0.59b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (0.59b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.59b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (0.59b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation==0.59b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (1.17.3)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-instrumentation-asgi==0.59b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio>=4.0.0) (2025.2)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=2.4.0->chromadb<0.5.0) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.1.16) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.1.16) (3.4.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0) (13.9.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0) (1.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers<3.0.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers<3.0.0) (3.6.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.5.0) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb<0.5.0) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.5.0) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb<0.5.0) (3.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "# ─── Cell 1: Install Packages  ─────────────────────────────────\n",
        "!pip install --upgrade \\\n",
        "  \"langchain==0.1.16\" \\\n",
        "  \"langchain-core==0.1.45\" \\\n",
        "  \"langchain-openai==0.1.3\" \\\n",
        "  \"openai>=1.0.0\" \\\n",
        "  \"gradio>=4.0.0\" \\\n",
        "  \"chromadb<0.5.0\" \\\n",
        "  \"sentence-transformers<3.0.0\" \\\n",
        "  \"pypdf<4.0.0\" \\\n",
        "  \"tiktoken\" \\\n",
        "  \"transformers<5.0.0\" \\\n",
        "  \"python-dotenv<1.1.0\" \\\n",
        "  \"pdf2image<1.17.0\" \\\n",
        "  \"pillow<11.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GLempZNjiVus",
        "outputId": "b1c39ce2-ce4f-4731-b86c-be45c816a23f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
            "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
          ]
        }
      ],
      "source": [
        "# ─── Cell 2: Imports & Setup  ─────────────────────────────────\n",
        "import os, json, io, base64\n",
        "from dotenv import load_dotenv\n",
        "from typing import List\n",
        "from PIL import Image\n",
        "from pdf2image import convert_from_path\n",
        "import gradio as gr\n",
        "\n",
        "from openai import OpenAI as OpenAIClient\n",
        "from langchain_openai import ChatOpenAI, OpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from pypdf import PdfReader\n",
        "\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zMTc7tY8-GP"
      },
      "source": [
        "### Step 2: Define the Tools\n",
        "**What this step does:**\n",
        "\n",
        "-   Defines three Python functions and decorates them with `@tool` so the agent can call them\n",
        "-   Each tool performs a specific task: parsing PDFs, rewriting text, or analyzing visual design\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **Tool**: A function the agent can invoke autonomously. The `@tool` decorator tells LangChain \"this function is available to the agent.\"\n",
        "-   **Docstring**: The description inside `\"\"\"triple quotes\"\"\"`. The LLM reads this to decide *when* to call the tool---make it clear and specific.\n",
        "-   **JSON-serializable output**: Tools must return simple Python types (dict, str, list, int) that can be converted to JSON---no raw objects or file handles.\n",
        "\n",
        "**The three tools:**\n",
        "\n",
        "1.  **`parse_resume(pdf_path, job_spec)`**\n",
        "    -   Extracts raw text from a PDF using `pypdf`\n",
        "    -   Returns a dict with `{\"text\": \"...\", \"job_spec\": \"...\"}`\n",
        "    -   **Why the job_spec parameter?** So the agent can thread context through multiple tool calls\n",
        "\n",
        "1.  **`enhance_resume(text, job_spec)`**\n",
        "    -   Rewrites résumé text to match the target role using a separate GPT-4 call\n",
        "    -   Uses `ChatOpenAI` (chat model) + `HumanMessage` wrapper\n",
        "    -   Returns improved text as a string\n",
        "\n",
        "1.  **`vision_style_analyzer(pdf_path)`**\n",
        "    -   Converts the first page of the PDF to a high-res image\n",
        "    -   Sends it to GPT-4 Vision (`gpt-4o`) with a critique prompt\n",
        "    -   Returns a structured JSON dict: `{\"style_score\": 7, \"template_type\": \"modern\", \"positive_points\": [...], \"improvement_suggestions\": [...]}`\n",
        "\n",
        "**Guardrails & best practices:**\n",
        "\n",
        "-   **Clear docstrings**: The agent can't read your mind---if the docstring says \"Extract résumé text,\" the agent knows to call this tool when the user uploads a PDF\n",
        "-   **Fail fast**: If a PDF path is invalid, `PdfReader` will raise an error---let it bubble up so the agent can report it\n",
        "-   **Type hints**: `pdf_path: str`, `job_spec: str` help with debugging and IDE autocomplete\n",
        "-   **JSON mode**: For `vision_style_analyzer`, we force `response_format={\"type\": \"json_object\"}` so GPT-4 Vision always returns valid JSON\n",
        "\n",
        "**Why split into three tools?**\n",
        "\n",
        "-   **Separation of concerns**: Parsing, rewriting, and visual analysis are distinct tasks\n",
        "-   **Reusability**: The agent can call `parse_resume` alone if the user only wants extraction\n",
        "-   **Observability**: You can debug each tool independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bF9XG6GOicm_"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 3: Tool 1 (parse_resume) ──────────────────────────────────────────\n",
        "@tool\n",
        "def parse_resume(pdf_path: str, job_spec: str) -> dict:\n",
        "    # This docstring is what the agent reads. Its very specific.\n",
        "    # It says Extract the raw text of a résumé PDF'\n",
        "    # The agent knows this tool is for PDF parsing, not image analysis or web scraping.\n",
        "    \"\"\"\n",
        "    Extract the raw text of a résumé PDF and carry along the job_spec.\n",
        "\n",
        "    Args:\n",
        "      pdf_path: Path to the uploaded PDF file\n",
        "      job_spec: Target job role/description (e.g., \"Software Engineer\")\n",
        "\n",
        "    Returns:\n",
        "      dict with keys 'text' (str) and 'job_spec' (str)\n",
        "    \"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "\n",
        "    # Specifies the shape: extract text from all pages, join with newlines\n",
        "    text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "\n",
        "    # Returns a JSON-serializable dict\n",
        "    return {\"text\": text, \"job_spec\": job_spec}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITJcoKC5Qtyz"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 4: Tool 2 (enhance_resume) ──────────────────────────────────────────\n",
        "@tool\n",
        "def enhance_resume(text: str, job_spec: str) -> str:\n",
        "    \"\"\"\n",
        "    Rewrite a résumé text for better clarity, impact, and relevance to the job_spec.\n",
        "\n",
        "    Args:\n",
        "      text: Raw résumé text (output from parse_resume)\n",
        "      job_spec: Target role (e.g., \"Product Manager\")\n",
        "\n",
        "    Returns:\n",
        "      Enhanced résumé text (str)\n",
        "    \"\"\"\n",
        "    prompt = f\"You are an expert résumé coach. Improve the bullets and wording of this résumé to match a {job_spec} role:{text}\"\n",
        "    chat_llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\") # changed from gpt-4-0613\n",
        "    response = chat_llm.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ruxKPStzIGrY"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 5: Tool 3 (vision_style_analyzer) ──────────────────────────────────────────\n",
        "@tool\n",
        "def vision_style_analyzer(pdf_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze the first page of the resume using GPT-4 Vision.\n",
        "\n",
        "    Returns a dict with:\n",
        "      - style_score (int): design score 1 (poor) to 10 (excellent)\n",
        "      - template_type (str): e.g. 'classic', 'modern', 'creative'\n",
        "      - positive_points (List[str]): what works visually\n",
        "      - improvement_suggestions (List[str]): concrete layout/design improvements\n",
        "    \"\"\"\n",
        "\n",
        "    # ============================================================================\n",
        "    # STEP 1: Convert PDF to Image\n",
        "    # ============================================================================\n",
        "    # Why? GPT-4 Vision can't read PDFs directly—it needs images.\n",
        "    # We convert ONLY the first page because:\n",
        "    #   - Résumés are judged by their first impression\n",
        "    #   - Converting multiple pages wastes memory/time\n",
        "    #   - Vision API charges per image\n",
        "    pages = convert_from_path(pdf_path, dpi=250, first_page=1, last_page=1)\n",
        "    img = pages[0].convert(\"RGB\")\n",
        "\n",
        "    # ============================================================================\n",
        "    # STEP 2: Resize if Needed (Vision Model Limit)\n",
        "    # ============================================================================\n",
        "    # Why? GPT-4 Vision has a 2048px width limit for \"high detail\" mode.\n",
        "    # If the image is wider, the API will auto-downscale it anyway—but we do it\n",
        "    # manually to control quality (LANCZOS is high-quality resampling).\n",
        "    w, h = img.size\n",
        "    if w > 2048:\n",
        "        img = img.resize((2048, int(2048 * h / w)), Image.LANCZOS)\n",
        "\n",
        "    # ============================================================================\n",
        "    # STEP 3: Compress to JPEG\n",
        "    # ============================================================================\n",
        "    # Why? Vision APIs accept base64-encoded images. PNG files can be huge\n",
        "    # (10+ MB for a full résumé page), causing:\n",
        "    #   - Slow uploads\n",
        "    #   - API timeouts\n",
        "    #   - Higher costs (OpenAI charges per token, images consume many tokens)\n",
        "    buf = io.BytesIO()\n",
        "    img.save(buf, format=\"JPEG\", quality=75)\n",
        "    buf.seek(0)\n",
        "\n",
        "\n",
        "    # ============================================================================\n",
        "    # STEP 4: Encode as Base64 Data URI\n",
        "    # ============================================================================\n",
        "    # Why? OpenAI's Vision API expects images as:\n",
        "    #   1. A public URL (e.g., https://example.com/image.jpg), OR\n",
        "    #   2. A base64 data URI (e.g., data:image/jpeg;base64,/9j/4AAQ...)\n",
        "    b64_img = base64.b64encode(buf.getvalue()).decode(\"ascii\")\n",
        "    image_url = f\"data:image/jpeg;base64,{b64_img}\"\n",
        "\n",
        "    # ============================================================================\n",
        "    # STEP 5: Call GPT-4 Vision API\n",
        "    # ============================================================================\n",
        "    # We use the native OpenAI client (not LangChain's ChatOpenAI) because:\n",
        "    #   - LangChain's vision support is less mature\n",
        "    #   - We need precise control over the \"detail\" parameter\n",
        "    client = OpenAIClient()\n",
        "\n",
        "    # --- Craft the Prompt ---\n",
        "    # This is CRITICAL. The prompt must:\n",
        "    #   1. Set clear expectations (\"brutally honest\")\n",
        "    #   2. Prevent generic advice (\"don't just say 'add white space'\")\n",
        "    #   3. Specify exact JSON structure (so we can parse it reliably)\n",
        "    prompt_text = (\n",
        "      \"You are a senior hiring manager at a top tech firm with a background in graphic design. \"\n",
        "      \"Your critique must be brutally honest and focused on what will get a candidate noticed or rejected based on visual presentation alone. \"\n",
        "      \"Analyze the attached résumé image and provide a detailed critique. \"\n",
        "      \"Do not give generic advice like 'add more white space' unless the document is genuinely cramped; instead, point to specific sections that need it. \"\n",
        "      \"Your response MUST be a single, raw JSON object with the following keys:\\n\"\n",
        "      \"  'style_score': An integer from 1-10 based on its immediate professional impact.\\n\"\n",
        "      \"  'template_type': One of 'classic', 'modern', or 'creative'.\\n\"\n",
        "      \"  'positive_points': An array of strings detailing what is visually effective (e.g., 'Good use of columns for readability').\\n\"\n",
        "      \"  'improvement_suggestions': An array of actionable, specific strings for visual improvement (e.g., 'The font size for section headers is inconsistent; make all headers 14pt').\\n\\n\"\n",
        "      \"Provide ONLY the raw JSON object and nothing else.\"\n",
        "    )\n",
        "\n",
        "    # --- Build the Messages Array ---\n",
        "    # OpenAI's vision API uses a nested structure:\n",
        "    #   messages = [\n",
        "    #       {\"role\": \"system\", \"content\": \"<instructions>\"},\n",
        "    #       {\"role\": \"user\", \"content\": [<text>, <image>, ...]}\n",
        "    #   ]\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert design critic providing feedback as a JSON object.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                # Part 1: Text prompt\n",
        "                {\"type\": \"text\", \"text\": prompt_text},\n",
        "\n",
        "                # Part 2: Image\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": image_url,\n",
        "                        \"detail\": \"high\"\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # --- Make the API Call ---\n",
        "    # guardrail: response_format={\"type\": \"json_object\"} forces the model to\n",
        "    # return ONLY valid JSON. If it returns prose, the API raises an error.\n",
        "    # This prevents us from trying to parse malformed output.\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # Using the more powerful model for better vision analysis\n",
        "        messages=messages,\n",
        "        temperature=0.5, # Giving it slightly more room for nuanced wording\n",
        "        response_format={\"type\": \"json_object\"} # Key guardrail: Forces JSON output\n",
        "    )\n",
        "\n",
        "    # 6. Parse and return\n",
        "    return json.loads(resp.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-d2lU0-EnV"
      },
      "source": [
        "### Step 3: Define the Instruction\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "-   Creates a `SystemMessage` that serves as the agent's instruction manual\n",
        "-   Defines the agent's role, available tools, and decision-making logic\n",
        "-   Establishes the workflow: which tools to call and in what order\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **SystemMessage**: A LangChain message type that sets the agent's identity and behavior (equivalent to `{\"role\": \"system\", \"content\": \"...\"}` in OpenAI's API)\n",
        "-   **Agent instructions**: The \"rulebook\" the LLM reads before every decision---tells it *who it is*, *what tools it has*, and *when to use them*\n",
        "-   **Tool-calling strategy**: The logic for chaining tools (e.g., \"Always call `parse_resume` first, then `enhance_resume`\")\n",
        "\n",
        "**Why this matters:**\n",
        "\n",
        "-   Without clear instructions, the agent will guess randomly which tool to call\n",
        "-   The system prompt is where you encode domain knowledge (e.g., \"résumés must be parsed before enhancement\")\n",
        "-   **The LLM cannot read your mind**---if you want the agent to call tools in a specific order, you must write it explicitly\n",
        "\n",
        "**Anatomy of this system prompt:**\n",
        "\n",
        "1.  **Identity**: \"You are the Résumé Enhancement Agent\"\\\n",
        "    → Sets the agent's persona and scope\n",
        "\n",
        "1.  **Tool catalog**: Lists each tool with signature and description\\\n",
        "    → Reinforces what the docstrings already say (redundancy is good---LLMs need repetition)\n",
        "\n",
        "1.  **Workflow rules**:\n",
        "    -   \"Always call `parse_resume` first\"\n",
        "    -   \"Use `enhance_resume` on the parsed text\"\n",
        "    -   \"Use `vision_style_analyzer` when the user asks for style critique\"\\\n",
        "        → Explicit conditionals guide the agent's decision tree\n",
        "\n",
        "**Guardrails & best practices:**\n",
        "\n",
        "-   **Be explicit**: Don't assume the agent will infer the workflow---write it out step-by-step\n",
        "-   **Use conditionals**: \"If the user asks X, do Y\" structures prevent tool misuse\n",
        "-   **Redundancy is good**: Even though tools have docstrings, restating their purpose in the system message reinforces correct behavior\n",
        "-   **Keep it concise**: Long prompts dilute focus---this one is ~150 words, enough for clarity without overwhelming the context window\n",
        "\n",
        "**Common pitfall:**\n",
        "\n",
        "-   **Vague instructions** like \"Help the user improve their résumé\" → the agent won't know whether to parse, enhance, or analyze first\n",
        "-   **Fix:** Write explicit steps: \"First parse, then enhance, then format the output\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YGhqruNNjqmV"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 6: Define the Instruction/System Prompt  ────────\n",
        "SYSTEM_MESSAGE = SystemMessage(\n",
        "    content=\"\"\"\n",
        "      You are the Résumé Enhancement Agent. You only “communicate” by calling one of these three functions:\n",
        "\n",
        "      1. parse_resume(pdf_path: str, job_spec: str) → {text, job_spec}\n",
        "        • Always call this first to extract raw text against the spec.\n",
        "\n",
        "      2. enhance_resume(text: str, job_spec: str) → {enhanced}\n",
        "        • Use this on the parsed text to rewrite/improve the résumé.\n",
        "\n",
        "      3. vision_style_analyzer(pdf_path: str) → {style_score, template_type, suggestions}\n",
        "        • Use this when the user asks for a style critique or template feedback.\n",
        "\n",
        "    Workflow rules:\n",
        "    If the user asks to critique the visual design or template of the résumé, call vision_style_analyzer.\n",
        "    If the user asks to enhance content, follow the chain: parse_resume → enhance_resume.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-xu9tFe0Qy_"
      },
      "source": [
        "#### Step 4: Initialise the Model\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "-   Sets the OpenAI API key securely using Colab Secrets\n",
        "-   Assembles the three tools into a list\n",
        "-   Creates a `ChatOpenAI` instance (the LLM \"brain\")\n",
        "-   Builds a `ChatPromptTemplate` that combines system instructions, user input, and conversation history\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **API key**: Your secret credential for accessing OpenAI's models---treat it like a password\n",
        "-   **`ChatOpenAI`**: LangChain's wrapper around OpenAI's chat models (GPT-4, GPT-3.5, etc.)\n",
        "-   **`temperature=0`**: Controls randomness; 0 = deterministic (same input → same output), 1 = creative/varied\n",
        "-   **`ChatPromptTemplate`**: A template that defines the structure of messages sent to the LLM\n",
        "-   **`MessagesPlaceholder`**: A dynamic slot in the prompt where LangChain inserts the agent's \"scratchpad\" (history of tool calls and results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur6tmSZU0P_H"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 7: Initialise the Model ──────────────────────────────────────────\n",
        "\n",
        "# 1. Set OpenAI API key from Colab Secrets\n",
        "# This sets an environment variable. LangChain's ChatOpenAI automatically looks for OPENAI_API_KEY in os.environ.\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 2. Assemble the tools list\n",
        "# Simple list.\n",
        "# The order doesn't matter here—the agent decides call order based on the system message, not this list.\n",
        "# But keep it consistent for readability.\n",
        "tools = [parse_resume, enhance_resume, vision_style_analyzer]\n",
        "\n",
        "# 3. Initialize the LLM\n",
        "# - model='gpt-4o': The latest GPT-4 variant with vision and function calling\n",
        "# - temperature=0: We set temperature to 0, which means the outputs are deterministic and consistent. \n",
        "# So every time you give it the same input, you get the same output. \n",
        "# This is crucial for production applications where you want predictable behavior.\n",
        "# If you want creative variation (e.g., multiple résumé rewrites),\n",
        "# use temperature=0.7, but for this workshop we want reproducibility.\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# 4. Build the prompt template\n",
        "# ChatPromptTemplate: A template that defines the structure of messages sent to the LLM\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SYSTEM_MESSAGE,\n",
        "    (\"human\", \"{input}\"),\n",
        "    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnjgr7Aa9XMy"
      },
      "source": [
        "#### Step 5: Initialise the AI Agent\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "-   Wires the LLM, tools, and prompt template into a functional agent\n",
        "-   Wraps the agent in an `AgentExecutor` that handles the execution loop, error handling, and logging\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **`create_openai_tools_agent`**: Factory function that binds tools to the LLM and returns an agent runnable\n",
        "-   **Agent**: The decision-making component---reads the prompt, decides which tool to call, formats the function call\n",
        "-   **`AgentExecutor`**: The runtime loop that executes the agent iteratively until the task is complete\n",
        "-   **`verbose=True`**: Enables detailed logging (shows each tool call, reasoning, and result in real-time)\n",
        "\n",
        "**How these two pieces work together:**\n",
        "\n",
        "1.  **The agent (created by `create_openai_tools_agent`):**\n",
        "    -   Takes the `llm`, `tools`, and `prompt` from Step 4\n",
        "    -   Automatically binds tools to the LLM (calls `llm.bind_tools(tools)` internally)\n",
        "    -   Returns a \"runnable\"---a LangChain object that can process one decision cycle\n",
        "    -   **Does NOT execute tools**---it only generates the decision (\"I should call `parse_resume` with these args\")\n",
        "\n",
        "1.  **The AgentExecutor (wraps the agent):**\n",
        "    -   Runs the agent in a loop:\n",
        "        1.  Invoke the agent → get a tool call decision\n",
        "        2.  Execute the tool → get the result\n",
        "        3.  Append result to scratchpad\n",
        "        4.  Invoke the agent again → repeat until done\n",
        "    -   Handles errors (e.g., if a tool raises an exception, the executor catches it and reports to the agent)\n",
        "    -   Enforces limits (default: max 15 iterations to prevent infinite loops)\n",
        "    -   Logs everything if `verbose=True`\n",
        "\n",
        "**Why separate the agent from the executor?**\n",
        "\n",
        "-   **Separation of concerns**: The agent handles *reasoning*, the executor handles *orchestration*\n",
        "-   **Testability**: You can test the agent's decision-making without executing tools\n",
        "-   **Configurability**: You can swap out executors (e.g., use a custom executor with rate limiting) without changing the agent\n",
        "\n",
        "**What happens when you run the agent:**\n",
        "\n",
        "\n",
        "```\n",
        "result = agent_executor.invoke({\"input\": \"Enhance my résumé for a PM role\"})\n",
        "```\n",
        "\n",
        "**Behind the scenes:**\n",
        "\n",
        "1.  **Iteration 1:**\n",
        "    -   AgentExecutor calls the agent with the prompt template filled in\n",
        "    -   Agent reads the system message and user input\n",
        "    -   Agent decides: \"I should call `parse_resume`\"\n",
        "    -   AgentExecutor executes `parse_resume(pdf_path, \"PM\")`\n",
        "    -   Result appended to scratchpad\n",
        "\n",
        "1.  **Iteration 2:**\n",
        "    -   AgentExecutor calls the agent again (with the updated scratchpad)\n",
        "    -   Agent reads: \"I called `parse_resume` and got text. Now I should call `enhance_resume`\"\n",
        "    -   AgentExecutor executes `enhance_resume(text, \"PM\")`\n",
        "    -   Result appended to scratchpad\n",
        "\n",
        "1.  **Iteration 3:**\n",
        "    -   Agent reads: \"I have the enhanced text. Task complete.\"\n",
        "    -   Agent generates a final response (not a tool call)\n",
        "    -   AgentExecutor returns the final output\n",
        "\n",
        "**Guardrails & best practices:**\n",
        "\n",
        "-   **Always use `verbose=True` during development** so you can see the agent's reasoning and debug issues\n",
        "-   **Set `max_iterations`** if you have expensive tools (default is 15, which is usually safe)\n",
        "-   **Handle errors gracefully**: If a tool fails, the executor will report the error to the agent, which can decide to retry or abort\n",
        "-   **The agent is stateless**: Each invocation starts fresh---if you want multi-turn conversations, you must pass the conversation history explicitly\n",
        "\n",
        "**Common pitfall:**\n",
        "\n",
        "-   **Calling the agent directly** (`agent.invoke(...)`) instead of the executor → the agent will generate tool calls but not execute them\n",
        "-   **Fix:** Always invoke the `AgentExecutor`, not the raw agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FE4Aqqm0daSu"
      },
      "outputs": [],
      "source": [
        "# ─── Cell 8: Initialise the AI Agent ──────────────────────────────────────────\n",
        "\n",
        "# 1. Create the agent using the modern 'create_openai_tools_agent' function\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# 2. Create the AgentExecutor, which will run the agent and tools in a loop\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLlHvP-F9tJ9"
      },
      "source": [
        "#### Step 6: Initialise the Interface\n",
        "\n",
        "**What this step does:**\n",
        "\n",
        "-   Creates a Gradio web interface with file upload, text input, and output display\n",
        "-   Wires two buttons (\"Enhance Résumé\" and \"Critique Style\") to the agent executor\n",
        "-   Launches a shareable web app for interacting with the AI agent\n",
        "\n",
        "**Key definitions:**\n",
        "\n",
        "-   **Gradio (`gr.Blocks`)**: A Python library for building ML/AI web UIs with minimal code\n",
        "-   **`gr.File`**: File upload component---returns a file object with `.name` (path on disk)\n",
        "-   **`gr.Textbox`**: Text input/output component\n",
        "-   **`gr.Button.click()`**: Connects a button to a Python function; when clicked, runs the function with the specified inputs and displays the result in outputs\n",
        "-   **`.queue().launch(share=True)`**: Starts the web server; `share=True` creates a public URL (valid for 72 hours)\n",
        "\n",
        "**Why Gradio for this project:**\n",
        "\n",
        "-   **Rapid prototyping**: 20 lines of code to go from working agent to working demo\n",
        "-   **No frontend knowledge needed**: Pure Python---no HTML, CSS, or JavaScript\n",
        "-   **Shareable**: `share=True` generates a public link you can send to colleagues or clients\n",
        "-   **Perfect for agents**: Gradio's simplicity lets us focus on the agent logic, not UI design\n",
        "\n",
        "**How the interface works:**\n",
        "\n",
        "1.  **User uploads a PDF** → Gradio saves it to `/tmp/gradio/...` and passes the file object to your function\n",
        "2.  **User enters a job spec** (e.g., \"Senior Data Scientist\") → stored in the `job_spec` textbox\n",
        "3.  **User clicks \"Enhance Résumé\"** → triggers the lambda function:\n",
        "\n",
        "```\n",
        "   lambda pdf, spec: run_agent(\n",
        "       f\"Please enhance the résumé at {pdf.name} for a {spec} position.\"\n",
        "   )\n",
        "```\n",
        "\n",
        "  -   `pdf.name` is the file path (e.g., `/tmp/gradio/abc123/resume.pdf`)\n",
        "  -   The lambda constructs a natural language query and passes it to `run_agent()`\n",
        "\n",
        "4.  **`run_agent()` calls the agent executor** → agent parses → enhances → returns result\n",
        "5.  **Result displayed in the output textbox**\n",
        "\n",
        "**The two workflows:**\n",
        "| Button | Query Template | Agent's Action |\n",
        "| --- | --- | --- |\n",
        "| **Enhance Résumé** | `\"Please enhance the résumé at {path} for a {spec} position.\"` | Calls `parse_resume` → `enhance_resume` → returns improved text |\n",
        "| **Critique Style** | `\"Please critique the visual design and template of the résumé at {path}.\"` | Calls `vision_style_analyzer` → returns JSON with score, type, suggestions |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "collapsed": true,
        "id": "ETRAfFN16JVI",
        "outputId": "d7e870c3-814b-44c9-914c-43bf18865123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9d73bdea619f5ceb2e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://9d73bdea619f5ceb2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ─── Cell 9: Initialise the Interface ──────────────────────────────────────────\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## AI‐Powered Résumé Enricher + Style Critic\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # This tells Gradio to accept any file and lets our Python code do the real validation.\n",
        "        resume_file = gr.File(label=\"Upload Résumé (PDF)\")\n",
        "        job_spec    = gr.Textbox(label=\"Job Specification\",\n",
        "                                 placeholder=\"e.g. Senior Software Engineer\")\n",
        "\n",
        "    btn_enhance = gr.Button(\"Enhance Résumé\")\n",
        "    btn_style   = gr.Button(\"Critique Style\")\n",
        "    output      = gr.Textbox(label=\"Result\", lines=20)\n",
        "\n",
        "    # Function to handle the agent invocation and extract the output\n",
        "    def run_agent(query):\n",
        "        response = agent_executor.invoke({\"input\": query})\n",
        "        return response['output']\n",
        "\n",
        "    # Launch enhancement flow\n",
        "    btn_enhance.click(\n",
        "        fn=lambda pdf, spec: run_agent(\n",
        "            f\"Please enhance the résumé at {pdf.name} for a {spec} position.\"\n",
        "        ),\n",
        "        inputs=[resume_file, job_spec],\n",
        "        outputs=output,\n",
        "    )\n",
        "\n",
        "    # Launch style-critique flow\n",
        "    btn_style.click(\n",
        "        fn=lambda pdf: run_agent(\n",
        "            f\"Please critique the visual design and template of the résumé at {pdf.name}.\"\n",
        "        ),\n",
        "        inputs=[resume_file],\n",
        "        outputs=output,\n",
        "    )\n",
        "\n",
        "    demo.queue().launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
